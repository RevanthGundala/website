<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello on Revanth Gundala</title>
    <link>/categories/hello/</link>
    <description>Recent content in Hello on Revanth Gundala</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 19 Feb 2026 09:16:45 +0000</lastBuildDate>
    <atom:link href="/categories/hello/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Trying to Make a VLA Its Own Reward Model</title>
      <link>/posts/trying-to-make-a-vla-its-own-reward-model/</link>
      <pubDate>Thu, 19 Feb 2026 09:16:45 +0000</pubDate>
      <guid>/posts/trying-to-make-a-vla-its-own-reward-model/</guid>
      <description>&lt;p&gt;&lt;em&gt;Code: &lt;a href=&#34;https://github.com/revanthgundala/siglip-grpo&#34;&gt;GitHub&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;figure&gt;&#xA;  &lt;img src=&#34;task0_ep1_success.gif&#34; alt=&#34;Successful robot trajectory&#34; loading=&#34;lazy&#34;&gt;&#xA;  &lt;figcaption&gt;A successful pick-and-place trajectory in LIBERO (reward: 1.0).&lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;h2 id=&#34;the-idea&#34;&gt;The idea&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2511.15605&#34;&gt;SRPO&lt;/a&gt; showed that a robot policy can improve itself by watching its own attempts. Starting at 48.9% success on LIBERO manipulation tasks, OpenVLA climbs to &lt;strong&gt;99.2%&lt;/strong&gt; through self-play. The trick is using &lt;a href=&#34;https://ai.meta.com/blog/v-jepa-2-video-understanding-model/&#34;&gt;V-JEPA 2&lt;/a&gt; — a 1.1B-parameter video encoder — to score how close each failure was to success. Near-misses get more reward than total failures, and the policy learns from that gradient via GRPO.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
